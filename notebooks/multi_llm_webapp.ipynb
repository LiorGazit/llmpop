{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWQF8tTX9hEF"
      },
      "source": [
        "# UX for LLMPop - Working Simultanously with Multiple LLMs, Locally, Safely, for Free\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LiorGazit/llmpop/blob/main/notebooks/multi_llm_webapp.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "This notebook is a minimal “click, pick, and prompt” UI for LLMPop that lets you select up to four models and compare their replies side by side.  \n",
        "It runs entirely free in your free Google Colab session and auto-handles local models via Ollama (no local installs on your machine).  \n",
        "Use the optional OpenAI API key only if you choose the gpt-4o tab, otherwise everything stays within your Colab runtime. \n",
        "It’s meant for quick demos and teaching, not production, so you can show friends how to use LLMs without touching code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7BCyEi-njg"
      },
      "source": [
        "### Setting up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPw6vdPJc4qg",
        "outputId": "ec60de5a-db34-4c36-bd8b-824d7a640429"
      },
      "outputs": [],
      "source": [
        "# =====================  ONE-CELL, SCHEMA-SAFE APP (HTML chats, string stores)  =====================\n",
        "# Add openai to ensure we can validate keys reliably.\n",
        "import sys, subprocess\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--upgrade\", \"gradio==4.44.1\", \"llmpop\", \"openai>=1.0.0\"],\n",
        "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True\n",
        ")\n",
        "\n",
        "import os, time, json, html\n",
        "os.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"false\"  # quieter logs\n",
        "\n",
        "import gradio as gr\n",
        "from llmpop import init_llm  # unified factory\n",
        "import openai\n",
        "\n",
        "VERBOSE = False\n",
        "if VERBOSE: print(\"Gradio version:\", gr.__version__)\n",
        "\n",
        "# -------------------- Constants & mappings --------------------\n",
        "TAB_COLORS = [\"#000000\", \"#0A3D91\", \"#006400\", \"#FF1493\"]  # black, dark blue, dark green, pink\n",
        "\n",
        "CHOICE_LABELS = [\n",
        "    \"llama3.2:1b — Free & local (XXS)\",\n",
        "    \"codellama — Free & local (S)\",\n",
        "    \"deepseek-r1 — Free & local (S)\",\n",
        "    \"gpt-4o — OpenAI (API key)\",\n",
        "]\n",
        "LABEL_TO_ID = {\n",
        "    CHOICE_LABELS[0]: \"llama3.2:1b\",\n",
        "    CHOICE_LABELS[1]: \"codellama\",\n",
        "    CHOICE_LABELS[2]: \"deepseek-r1\",\n",
        "    CHOICE_LABELS[3]: \"gpt-4o\",\n",
        "}\n",
        "DEFAULT_LABELS = CHOICE_LABELS[:]\n",
        "\n",
        "def make_status(msg, color=None):\n",
        "    clr = color or \"#444\"\n",
        "    return f'<div style=\"font-family:system-ui,sans-serif;font-size:14px;color:{clr};\">{msg}</div>'\n",
        "\n",
        "def render_chat(history, slot_idx):\n",
        "    \"\"\"history = list of {'role','content'} dicts -> colored HTML.\"\"\"\n",
        "    color_user = \"#333\"\n",
        "    color_assistant = TAB_COLORS[slot_idx]\n",
        "    rows = []\n",
        "    for m in history:\n",
        "        role = m.get(\"role\", \"\")\n",
        "        content = html.escape(m.get(\"content\", \"\"))\n",
        "        if role == \"user\":\n",
        "            rows.append(f'<div style=\"margin:6px 0;padding:8px 10px;border-radius:10px;background:#f1f5f9;color:{color_user};\"><b>You:</b> {content}</div>')\n",
        "        else:\n",
        "            rows.append(f'<div style=\"margin:6px 0;padding:8px 10px;border-radius:10px;background:#fff0; border:1px solid {color_assistant};color:{color_assistant};\"><b>Model:</b> {content}</div>')\n",
        "    return \"<div>\" + \"\".join(rows) + \"</div>\"\n",
        "\n",
        "# -------------------- Model cache & helpers --------------------\n",
        "_model_cache = {}\n",
        "\n",
        "def get_model(model_name, provider, api_key, status_acc, color):\n",
        "    key = (model_name, provider, api_key or \"\")\n",
        "    if key in _model_cache:\n",
        "        return _model_cache[key]\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        if not api_key:\n",
        "            status_acc.append(make_status(f\"OpenAI model '{model_name}' selected but no API key provided.\", \"#B00020\"))\n",
        "            return None\n",
        "        status_acc.append(make_status(f\"Initializing OpenAI model: {model_name}…\", color))\n",
        "        mdl = init_llm(model=model_name, provider=\"openai\",\n",
        "                       provider_kwargs={\"api_key\": api_key}, temperature=0.0, verbose=False)\n",
        "        status_acc.append(make_status(f\"Ready: {model_name}\", color))\n",
        "    else:\n",
        "        status_acc.append(make_status(\n",
        "            f\"Starting/connecting to Ollama and pulling '{model_name}' (first time takes a bit)…\", color\n",
        "        ))\n",
        "        mdl = init_llm(model=model_name, provider=\"ollama\",\n",
        "                       provider_kwargs={\"pull\": True, \"auto_install\": True, \"auto_serve\": True},\n",
        "                       temperature=0.0, verbose=False)\n",
        "        status_acc.append(make_status(f\"Ready: {model_name}\", color))\n",
        "\n",
        "    _model_cache[key] = mdl\n",
        "    return mdl\n",
        "\n",
        "def append_turn(history, user, assistant):\n",
        "    history = history or []\n",
        "    history += [{\"role\": \"user\", \"content\": user},\n",
        "                {\"role\": \"assistant\", \"content\": assistant}]\n",
        "    return history\n",
        "\n",
        "def start_stream_turn(history, user_msg):\n",
        "    \"\"\"Append a user message and an empty assistant message to stream into.\"\"\"\n",
        "    history = history or []\n",
        "    history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "    history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
        "    return history\n",
        "\n",
        "def extend_last_assistant(history, delta):\n",
        "    \"\"\"Append streamed text to the *last* assistant message in history.\"\"\"\n",
        "    if history and history[-1].get(\"role\") == \"assistant\":\n",
        "        history[-1][\"content\"] = (history[-1][\"content\"] or \"\") + (delta or \"\")\n",
        "    return history\n",
        "\n",
        "# -------------------- OpenAI API key validation (used only by backend guard) --------------------\n",
        "def check_openai_api_key_valid(key: str) -> bool:\n",
        "    \"\"\"Support both OpenAI Python v1 client and the legacy style, without polluting global state.\"\"\"\n",
        "    key = (key or \"\").strip()\n",
        "    if not key:\n",
        "        return False\n",
        "    try:\n",
        "        # v1+ style\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            client = OpenAI(api_key=key)\n",
        "            _ = client.models.list()\n",
        "            return True\n",
        "        except Exception:\n",
        "            # Legacy fallback\n",
        "            try:\n",
        "                old_key = getattr(openai, \"api_key\", None)\n",
        "                openai.api_key = key\n",
        "                _ = openai.Model.list()\n",
        "                openai.api_key = old_key  # restore\n",
        "                return True\n",
        "            except Exception as _e:\n",
        "                return False\n",
        "    except openai.AuthenticationError:\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        # Network or env issues — treat as invalid for gating; log for notebook visibility\n",
        "        print(f\"[Key check] Unexpected error: {e}\", flush=True)\n",
        "        return False\n",
        "\n",
        "# -------------------- UI --------------------\n",
        "with gr.Blocks(\n",
        "    title=\"LLMPop — Click · Pick · Prompt (Colab)\",\n",
        "    theme=gr.themes.Default(),\n",
        "    css=f\"\"\"\n",
        "    .llm-tab-0 .tabitem .tabs-label {{ color: {TAB_COLORS[0]} !important; }}\n",
        "    .llm-tab-1 .tabitem .tabs-label {{ color: {TAB_COLORS[1]} !important; }}\n",
        "    .llm-tab-2 .tabitem .tabs-label {{ color: {TAB_COLORS[2]} !important; }}\n",
        "    .llm-tab-3 .tabitem .tabs-label {{ color: {TAB_COLORS[3]} !important; }}\n",
        "    .slot-header-0 {{ color:{TAB_COLORS[0]}; font-weight:700; }}\n",
        "    .slot-header-1 {{ color:{TAB_COLORS[1]}; font-weight:700; }}\n",
        "    .slot-header-2 {{ color:{TAB_COLORS[2]}; font-weight:700; }}\n",
        "    .slot-header-3 {{ color:{TAB_COLORS[3]}; font-weight:700; }}\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Experiment with local LLMs in your private environment\\nPick LLMs on the left, tick which to invoke, then prompt below.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # ---- Left panel ----\n",
        "        with gr.Column(scale=1, min_width=320):\n",
        "            gr.Markdown(\"### LLM picks\")\n",
        "            model_dd, model_ck = [], []\n",
        "\n",
        "            for i, default_label in enumerate(DEFAULT_LABELS):\n",
        "                with gr.Row():\n",
        "                    dd = gr.Dropdown(\n",
        "                        choices=CHOICE_LABELS,\n",
        "                        value=default_label,\n",
        "                        label=f\"Slot {i+1} model\",\n",
        "                        allow_custom_value=False,\n",
        "                        filterable=False,\n",
        "                    )\n",
        "                    ck = gr.Checkbox(value=False, label=\"Invoke on next prompt\")\n",
        "                model_dd.append(dd)\n",
        "                model_ck.append(ck)\n",
        "\n",
        "            api_key = gr.Textbox(\n",
        "                label=\"OpenAI API Key (optional, for GPT-4o)\",\n",
        "                placeholder=\"sk-…\",\n",
        "                type=\"password\",\n",
        "                lines=1,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Status\")\n",
        "            status_box = gr.HTML(make_status(\"Idle.\", \"#666\"))\n",
        "\n",
        "        # ---- Right panel ----\n",
        "        with gr.Column(scale=2, min_width=680):\n",
        "            tabs = gr.Tabs()\n",
        "            slot_headers = []\n",
        "            conv_html = []      # HTML render of chat per slot\n",
        "            hist_store = []     # hidden string stores (JSON list of messages)\n",
        "\n",
        "            for i in range(4):\n",
        "                with gr.Tab(f\"LLM Slot {i+1}\", elem_classes=[f\"llm-tab-{i}\"]):\n",
        "                    header = gr.HTML(f'<div class=\"slot-header-{i}\">Model: <b>{DEFAULT_LABELS[i]}</b></div>')\n",
        "                    html_box = gr.HTML(\"\")   # output-only chat render\n",
        "                    hidden_store = gr.Textbox(value=\"[]\", visible=False)  # stringified JSON\n",
        "\n",
        "                    slot_headers.append(header)\n",
        "                    conv_html.append(html_box)\n",
        "                    hist_store.append(hidden_store)\n",
        "\n",
        "            prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Type your prompt here…\", lines=6)\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "\n",
        "    # Header updates & tab reset (clear both chat render + store)\n",
        "    def header_html(label, idx):\n",
        "        return f'<div class=\"slot-header-{idx}\">Model: <b>{label}</b></div>'\n",
        "\n",
        "    for i, dd in enumerate(model_dd):\n",
        "        dd.change(fn=lambda label, idx=i: header_html(label, idx), inputs=dd, outputs=slot_headers[i])\n",
        "        dd.change(fn=lambda: \"\", inputs=None, outputs=conv_html[i])\n",
        "        dd.change(fn=lambda: \"[]\", inputs=None, outputs=hist_store[i])\n",
        "\n",
        "    # -------------------- Send (STREAMING) --------------------\n",
        "    def send(\n",
        "        p_text,\n",
        "        h1, h2, h3, h4,            # stringified JSON histories\n",
        "        l1, l2, l3, l4,            # dropdown labels\n",
        "        c1, c2, c3, c4,            # checkboxes\n",
        "        openai_key\n",
        "    ):\n",
        "        # parse stores -> lists\n",
        "        def parse_hist(s):\n",
        "            try:\n",
        "                v = json.loads(s or \"[]\")\n",
        "                return v if isinstance(v, list) else []\n",
        "            except Exception:\n",
        "                return []\n",
        "        histories = [parse_hist(h1), parse_hist(h2), parse_hist(h3), parse_hist(h4)]\n",
        "        labels    = [l1, l2, l3, l4]\n",
        "        checks    = [c1, c2, c3, c4]\n",
        "\n",
        "        def emit(status_html):\n",
        "            # outputs: conv_html(4), hist_store(4), status\n",
        "            return (\n",
        "                render_chat(histories[0], 0),\n",
        "                render_chat(histories[1], 1),\n",
        "                render_chat(histories[2], 2),\n",
        "                render_chat(histories[3], 3),\n",
        "                json.dumps(histories[0]),\n",
        "                json.dumps(histories[1]),\n",
        "                json.dumps(histories[2]),\n",
        "                json.dumps(histories[3]),\n",
        "                status_html\n",
        "            )\n",
        "\n",
        "        # Early validations — YIELD so the UI updates immediately\n",
        "        if not any(checks):\n",
        "            yield emit(make_status(\"No LLM selected. Tick at least one checkbox to invoke.\", \"#B00020\"))\n",
        "            return\n",
        "        if not p_text or not p_text.strip():\n",
        "            yield emit(make_status(\"Please enter a prompt.\", \"#B00020\"))\n",
        "            return\n",
        "\n",
        "        statuses = []\n",
        "\n",
        "        for idx in range(4):\n",
        "            if not checks[idx]:\n",
        "                continue\n",
        "\n",
        "            label = labels[idx]\n",
        "            model_id = LABEL_TO_ID.get(label)\n",
        "            if not model_id:\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] Invalid model selection.\", \"#B00020\"))\n",
        "                yield emit(\"\".join(statuses))\n",
        "                continue\n",
        "\n",
        "            provider = \"openai\" if model_id == \"gpt-4o\" else \"ollama\"\n",
        "            color = TAB_COLORS[idx]\n",
        "\n",
        "            if VERBOSE: print(f\"[Slot {idx+1}] Selected: {model_id} via {provider}\", flush=True)\n",
        "\n",
        "            # Backend guard: if GPT-4o is selected, require a valid key\n",
        "            if provider == \"openai\":\n",
        "                if not (openai_key or \"\").strip() or not check_openai_api_key_valid(openai_key):\n",
        "                    statuses.append(make_status(f\"[Slot {idx+1}] '{model_id}' selected but no valid OpenAI API key.\", \"#B00020\"))\n",
        "                    yield emit(\"\".join(statuses))\n",
        "                    continue\n",
        "\n",
        "            # Prep step\n",
        "            statuses.append(make_status(f\"[Slot {idx+1}] Preparing {model_id}…\", color))\n",
        "            yield emit(\"\".join(statuses))\n",
        "            if VERBOSE: print(f\"[Slot {idx+1}] Preparing {model_id}…\", flush=True)\n",
        "\n",
        "            # Build/cache model (adds its own status lines)\n",
        "            try:\n",
        "                mdl = get_model(model_id, provider, (openai_key or \"\").strip() or None, statuses, color)\n",
        "                yield emit(\"\".join(statuses))\n",
        "                if mdl is None:\n",
        "                    continue\n",
        "            except Exception as e:\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] Init error: {e!s}\", \"#B00020\"))\n",
        "                yield emit(\"\".join(statuses))\n",
        "                if VERBOSE: print(f\"[Slot {idx+1}] Init ERROR: {e!s}\", flush=True)\n",
        "                continue\n",
        "\n",
        "            # Start streamed turn: add user + empty assistant\n",
        "            histories[idx] = start_stream_turn(histories[idx], p_text)\n",
        "            statuses.append(make_status(f\"[Slot {idx+1}] Invoking {model_id}… (streaming)\", color))\n",
        "            yield emit(\"\".join(statuses))\n",
        "            if VERBOSE: print(f\"[Slot {idx+1}] Invoking {model_id} (streaming)…\", flush=True)\n",
        "\n",
        "            # Stream tokens/chunks\n",
        "            t0 = time.time()\n",
        "            token_count = 0\n",
        "            try:\n",
        "                if hasattr(mdl, \"stream\"):\n",
        "                    for chunk in mdl.stream(p_text):\n",
        "                        piece = getattr(chunk, \"content\", \"\") or \"\"\n",
        "                        if not piece:\n",
        "                            continue\n",
        "                        token_count += 1\n",
        "                        histories[idx] = extend_last_assistant(histories[idx], piece)\n",
        "\n",
        "                        # Throttle UI refreshes for performance\n",
        "                        if token_count % 12 == 0:\n",
        "                            yield emit(\"\".join(statuses) + make_status(f\"[Slot {idx+1}] streamed {token_count} tokens…\", color))\n",
        "                    dt = time.time() - t0\n",
        "                else:\n",
        "                    out = mdl.invoke(p_text).content\n",
        "                    histories[idx] = extend_last_assistant(histories[idx], out)\n",
        "                    dt = time.time() - t0\n",
        "\n",
        "                statuses.append(make_status(\n",
        "                    f\"[Slot {idx+1}] {model_id} finished in {dt:.1f}s.\" + (f\" ({token_count} tokens streamed)\" if token_count else \"\"),\n",
        "                    color\n",
        "                ))\n",
        "                yield emit(\"\".join(statuses))\n",
        "                if VERBOSE: print(f\"[Slot {idx+1}] Done in {dt:.1f}s, tokens={token_count}\", flush=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] {model_id} error: {e!s}\", \"#B00020\"))\n",
        "                yield emit(\"\".join(statuses))\n",
        "                if VERBOSE: print(f\"[Slot {idx+1}] ERROR: {e!s}\", flush=True)\n",
        "\n",
        "        if not statuses:\n",
        "            yield emit(make_status(\"Done.\", \"#666\"))\n",
        "\n",
        "    # Button wiring: inputs = prompt + *string stores* + dropdowns + checkboxes + key\n",
        "    send_btn.click(\n",
        "        fn=send,\n",
        "        inputs=[prompt, *hist_store, *model_dd, *model_ck, api_key],\n",
        "        outputs=[*conv_html, *hist_store, status_box],\n",
        "        concurrency_limit=1,\n",
        "    )\n",
        "\n",
        "# Queue + Launch\n",
        "_ = demo.queue(max_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPVv9nA4-hOG"
      },
      "source": [
        "### Running the app:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "DAkXYTbA-cQb",
        "outputId": "d421f872-bf17-4d1f-a72c-8146fff27c02"
      },
      "outputs": [],
      "source": [
        "# --- Launch silently and print only the final URL ---\n",
        "import io, contextlib, time\n",
        "\n",
        "buf = io.StringIO()\n",
        "with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
        "    server = demo.launch(\n",
        "        share=True,\n",
        "        inline=False,           # don't embed UI\n",
        "        quiet=True,             # reduce gradio chatter\n",
        "        show_api=False,         # no schema dump\n",
        "        inbrowser=False,\n",
        "        max_threads=1,\n",
        "        prevent_thread_lock=True,  # return immediately\n",
        "    )\n",
        "\n",
        "    # Poll for the share URL to be set (gradio sets it asynchronously)\n",
        "    link = None\n",
        "    deadline = time.time() + 45  # seconds\n",
        "    while time.time() < deadline:\n",
        "        # Try server first, then the demo object\n",
        "        link = (\n",
        "            getattr(server, \"share_url\", None)\n",
        "            or getattr(server, \"local_url\", None)\n",
        "            or getattr(demo, \"share_url\", None)\n",
        "            or getattr(demo, \"local_url\", None)\n",
        "        )\n",
        "        if link:\n",
        "            break\n",
        "        time.sleep(0.25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Click the link for the UI:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "print(\"Click on the link to open the LLM app in a new tab:\")\n",
        "# Print only the link (or a clear fallback message)\n",
        "print(link if link else \"App started (no URL available yet)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WWQF8tTX9hEF"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
